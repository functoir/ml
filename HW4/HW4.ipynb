{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 74: Machine Learning\n",
    "\n",
    "## Homework  4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn import naive_bayes\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Data and Partition it\n",
    "\n",
    "80% of the data is used for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  Label\n",
      "0    7430.14    9529.78   -2453.33         19        123        621      0\n",
      "1   11256.40   50455.10   -4220.00         18        216       2677      0\n",
      "2   13093.00   51897.10   -2880.00         30        234       2464      0\n",
      "3   14303.00  102632.00   -5702.20        144        281       4061      1\n",
      "4   14688.00   83343.40   -2430.00         52        223       2822      1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"hw4_naive.csv\")\n",
    "print(df.head())\n",
    "\n",
    "dataset = np.array(df)\n",
    "\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multinomial NaivÃ« Bayes classifier with Smooting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_bayes.MultinomialNB()\n",
    "\n",
    "class NaiveBayes():\n",
    "    \"\"\"\n",
    "    Naive Bayes Predictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainset=None, smoothing_coeff=1):\n",
    "        \"\"\"\n",
    "            Initializes the Naive Bayes predictor.\n",
    "\n",
    "            Inputs:\n",
    "            -------\n",
    "                `trainset`: 2D numpy array of training data.\n",
    "                    >>> NOTE: The last column of the data is the class label.\n",
    "                `smoothing_coeff`: float, smoothing coefficient\n",
    "                        for multinomial classifier.\n",
    "                        Default value = 1.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.possible_classes = list(np.unique(trainset[:, -1]))\n",
    "        self.trainset: npt.NDArray = trainset\n",
    "        self.smoothing_coeff: float = smoothing_coeff\n",
    "        self.feature_count: int = self.trainset.shape[1] - 1\n",
    "\n",
    "        # these variables are used by the gaussian classifier\n",
    "        self.gaussian_means = None\n",
    "        self.gaussian_variance = None\n",
    "\n",
    "    def multinomial(self, feature_vector):\n",
    "        \"\"\"\n",
    "            Calculates the probability of each class given the data.\n",
    "\n",
    "            Inputs:\n",
    "            -------\n",
    "\n",
    "            `feature_vector`: 1D numpy array for aa single row of data.\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            A classification for the feature vector,\n",
    "            based on the priorly saved X_train and y_train.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        class_count = len(self.possible_classes)\n",
    "        classes = np.array([0 for _ in range(class_count)])\n",
    "        class_totals = np.array([0 for _ in self.possible_classes])\n",
    "        for record in self.trainset:\n",
    "            label = record[-1]\n",
    "            class_index = self.possible_classes.index(label)\n",
    "            for feature in range(self.feature_count):\n",
    "                if record[feature] == feature_vector[feature]:\n",
    "                    classes[class_index] += 1\n",
    "\n",
    "                class_totals[class_index] += 1\n",
    "\n",
    "        probabilities = (classes + self.smoothing_coeff) / (class_totals + class_count)\n",
    "        return self.possible_classes[np.argmax(probabilities)]\n",
    "\n",
    "    def multinomial_predictions(self, feature_vectors):\n",
    "        \"\"\"\n",
    "            Calculates the multinomial predictions for each record in a matrix.\n",
    "\n",
    "            Inputs:\n",
    "            -------\n",
    "                `feature_vectors`: 2D numpy array of feature vectors.\n",
    "            \n",
    "            Outputs:\n",
    "            --------\n",
    "                `predictions`: 1D list of predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.apply_along_axis(self.multinomial, 1, feature_vectors)\n",
    "    \n",
    "    def gaussian(self, feature_vector):\n",
    "        \"\"\"\n",
    "            Calculates the probability of each class given the data.\n",
    "\n",
    "            Inputs:\n",
    "            -------\n",
    "\n",
    "            `feature_vector`: 1D numpy array for aa single row of data.\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            A classification for the feature vector,\n",
    "            based on the priorly saved X_train and y_train.\n",
    "\n",
    "        \"\"\"\n",
    "        if feature_vector.shape[0] == self.feature_count + 1:\n",
    "            feature_vector = feature_vector[:-1]\n",
    "\n",
    "\n",
    "        # if means and variance are not initialized, initialize them\n",
    "        if self.gaussian_means is None:\n",
    "            self.gaussian_means = []\n",
    "            self.gaussian_variance = []\n",
    "\n",
    "            for possible_class in self.possible_classes:\n",
    "                class_data = self.trainset[self.trainset[:, -1] == possible_class]\n",
    "                class_mean = np.mean(class_data[:, :-1], axis=0)\n",
    "                class_variance = np.var(class_data[:, :-1], axis=0)\n",
    "                self.gaussian_means.append(class_mean)\n",
    "                self.gaussian_variance.append(class_variance)\n",
    "\n",
    "        probabilities = []\n",
    "        for possible_class in range(len(self.possible_classes)):\n",
    "            class_mean = self.gaussian_means[possible_class]\n",
    "            class_variance = self.gaussian_variance[possible_class]\n",
    "            prob = (1 / (np.sqrt(2 * np.pi * class_variance)))\n",
    "            prob *= np.exp(-( (feature_vector - class_mean) ** 2) / (2 * class_variance))\n",
    "            probabilities.append(np.product(prob))\n",
    "            \n",
    "        return self.possible_classes[np.argmax(probabilities)]\n",
    "\n",
    "    def gaussian_predictions(self, feature_vectors):\n",
    "        \"\"\"\n",
    "            Calculates the multinomial predictions for each record in a matrix.\n",
    "\n",
    "            Inputs:\n",
    "            -------\n",
    "                `feature_vectors`: 2D numpy array of feature vectors.\n",
    "            \n",
    "            Outputs:\n",
    "            --------\n",
    "                `predictions`: 1D list of predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.apply_along_axis(self.gaussian, 1, feature_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Multinomial Naive Bayes classifier with Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.trainset.shape = (4480, 7)\n",
      "self.possible_classes = [0.0, 1.0]\n",
      "multinomial_predictions = array([1., 0., 0., ..., 0., 0., 0.])\n",
      "correct = 865, total = 1120\n",
      "precision = 0.8523076923076923\n",
      "recall = 0.5723140495867769\n",
      "f1 = 0.684796044499382\n",
      "accuracy = 0.7723214285714286\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(trainset)\n",
    "multinomial_predictions = nb.multinomial_predictions(testset)\n",
    "\n",
    "# print(f\"{multinomial_predictions = }\")\n",
    "\n",
    "# check the accuracy of the predictions\n",
    "count = len(multinomial_predictions)\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "for i in range(count):\n",
    "    prediction = multinomial_predictions[i]\n",
    "    if prediction == testset[i][-1]:\n",
    "        if prediction == 1:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            true_negative += 1\n",
    "    else:\n",
    "        if prediction == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "correct = true_positive + true_negative\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f\"{correct = }, total = {count}\")\n",
    "print(f\"{precision = }\\n{recall = }\\n{f1 = }\")\n",
    "print(f\"accuracy = {correct / count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. ... 0. 1. 1.]\n",
      "correct = 666, total = 1120\n",
      "precision = 0.5707547169811321\n",
      "recall = 0.25\n",
      "f1 = 0.34770114942528735\n",
      "accuracy = 0.5946428571428571\n"
     ]
    }
   ],
   "source": [
    "# nb = NaiveBayes(trainset)  # if initialized above, no need to initialize here.\n",
    "gaussian_predictions = nb.gaussian_predictions(testset)\n",
    "\n",
    "# print(gaussian_predictions)\n",
    "\n",
    "# check the accuracy of the predictions\n",
    "count = len(gaussian_predictions)\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "for i in range(count):\n",
    "    prediction = gaussian_predictions[i]\n",
    "    if prediction == testset[i][-1]:\n",
    "        if prediction == 1:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            true_negative += 1\n",
    "    else:\n",
    "        if prediction == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "correct = true_positive + true_negative\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f\"{correct = }, total = {count}\")\n",
    "print(f\"{precision = }\\n{recall = }\\n{f1 = }\")\n",
    "print(f\"accuracy = {correct / count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus Questions: Clustering\n",
    "\n",
    "- Given a [training dataset](./hw4_cluster.csv) containing 40 rows, each with 2 columns.\n",
    "- Column 1 & 2 are the features.\n",
    "- There are no labels for this dataset.\n",
    "- Implement different clustering algorithms and run them on this\n",
    "dataset. \n",
    "- **Assume the distance function is _Euclidean Distance_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) (35 points) Implement a generalized K-means/median algorithm.\n",
    "\n",
    "  - You should have a single function that takes in as input the data points, K,\n",
    "    and some other hyperparameters specified below.\n",
    "  - The function should return K sets of data points, each set corresponding to one cluster."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "607b7d84c7d8e26dbbffb4014e40424fe2faf80a09a85d717e93e42c2773dc40"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
