# -*- coding: utf-8 -*-
"""HW1_cosc74.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rBDmlh-e4BoQwH40JO9sw0UZ1f1BXvq7

# COSC 74: Machine Learning

## Homework 1

### Student: Amittai Wekesa

### Date: April 21, 2022

You should submit this jupyter notebook with your solutions. The solutions should include the code and also the output of all the cells.

Note that for the problmes that require a cost function as input you should always use the most recent cost function that you have implemented (unless specified otherwise).

1) [5 points] Calculate the derivative of following cost function and write it down:

$g(w) = \frac{1}{50}\left(w^4 + w^2 + 10w - 50 \right)$

$\frac{\partial}{\partial w}g(w) = \frac{1}{50}(4w^3 + 2w + 10) = \frac{1}{25}(2w^3 + w + 5)$

---

2) [25 points] Implement the gradient descent function as discussed in class using the gradient derived in the last problem. The function should return the cost history for each step. Use the code template below:
"""

#gradient descent function
#inputs: alpha (learning rate parameter), max_its (maximum number of iterations), w0 (initialization)
def gradient_descent(alpha, max_its: int, w0) -> list:
    """
        Function to run gradient descent,
        given a starting weight and a number of iterations.\n
        `alpha`: learning rate
        `max_its`: number of iterations
        `w0`: initial weight 
        `returns`: list of weights after each iteration step.   
    """
    
    # 0. init
    w = w0
    cost_history = [g(w)]
    
    # 1. iterate
    for _ in range(max_its):
        
        # 2. step
        w -= alpha * dw(w)
        
        # 3. record
        cost_history.append(g(w))
        
    return cost_history

def g(w) : return 1/50 * (w**4 + w**2 + 10 * w - 50)
def dw(w) : return 1/50 * (4 * w**3 + 2 * w + 10)


# gradient_descent(3, 10, 2.0)

"""---

3) [10 points] Run the gradient_descent function you implemented three times, with the following parameters. Generate a single plot showing the cost as a function of step number for all three runs (combine all three runs into a single plot). If you are not familiar with plotting in python, here is the docs for matplotlib:(https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot). 


$w^0$ = 2.0
max_its = 1000

1. Alpha = 1
2. Alpha = 0.1
3. Alpha = 0.01

"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline

max_steps = 1000
steps = list(range(max_steps + 1))
alpha_1 = gradient_descent(1, 1000, 2.0)
alpha_2 = gradient_descent(0.1, 1000, 2.0)
alpha_3 = gradient_descent(0.01, 1000, 2.0)

plt.plot(steps, alpha_1, color="green", label='alpha=1')
plt.plot(steps, alpha_2, color="blue", label='alpha=0.1')
plt.plot(steps, alpha_3, color="red",label='alpha=0.01')

plt.xticks(np.arange(-50, 1010, 50))
plt.yticks(np.arange(-1.2, 0.2, 0.2))
plt.grid(linestyle="--")

plt.legend(loc="upper right")

plt.show()

"""---

For the next few problems we will be comparing fixed and diminishing learning rates

Take the following cost function:
\begin{equation}
g(w) = \left \vert w \right \vert
\end{equation}

4) [5 points] Is this function convex? If no, why not? If yes, where is its global minimum?

Yes, the function is convex.

$$
g(w) = \left \vert w \right \vert = \begin{cases}
  -w & \text{if } w < 0 \\
  0 & \text{if } w = 0 \\
  w & \text{if } w > 0
\end{cases}
$$

The global minimum occurs at $w = 0$
"""

import numpy as np
import matplotlib.pyplot as plt
x, f_x = [], []
for i in np.arange(-10.0, 10.0, 0.1):
    x.append(i)
    f_x.append(np.abs(i))

plt.plot(x, f_x, color="red", label='y = abs(x)')

plt.xticks(np.arange(-11, 11, 1))
plt.yticks(np.arange(-1, 12, 1))
plt.grid(linestyle="--")

plt.legend(loc="lower right")

plt.show()

"""---

5) [5 points] What is the derivative of the cost function?

$$
g'(w) = \frac{w}{\left \vert w \right \vert} = \begin{cases}
  -1 & \text{if } w < 0 \\
  \mathbf{undefined} & \text{if } w = 0 \\
  1 & \text{if } w > 0
\end{cases}
$$
"""

import numpy as np
import matplotlib.pyplot as plt
from autograd import grad
gradient = grad(np.abs)
x, f_x = [], []
for i in np.arange(-10.0, 10.0, 0.1):
    x.append(i)
    f_x.append(gradient(i))

plt.plot(x, f_x, color="red", label='y = grad(abs)(x)')

plt.xticks(np.arange(-11, 11, 1))
plt.yticks(np.arange(-2, 3, 1))
plt.grid(linestyle="--")

plt.legend(loc="lower right")

plt.show()

"""---

6) [20 points] Rewrite the gradient descent function from question 2 such that it takes the cost function g as input and uses the autograd library to calculate the gradient. The function should return the weight and cost history for each step. Use the code template below.

autograd is a python package for automatic calculation of the gradient. Here is a tutorial on it: (http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/tutorials/tut4.pdf

Note that in Python you can pass functions around like any other variables. That is why you can pass the cost function g to the gradient_descent function. 

You should be able to install it by running "pip install autograd" in a cell in your Jupyter notebook.
"""

from autograd import grad
import autograd.numpy as np

def gradient_descent(g, alpha, max_its, w0):
    """
        Gradient Descent.\n
        `g`: cost function.
        `alpha`: learning rate
        `max_its`: number of iterations
        `w0`: initial weight 
        `returns`: lists of weights and corresponding costs
        costs after each iteration step.   
    """
    
    # 0. init
    gradient = grad(g)
    w = w0
    weight_history = [w]
    cost_history = [g(w)]
    
    # 1. iterate
    for step in range(max_its):

        # 2. step
        w -= alpha * gradient(w)
        
        # 3. record
        cost_history.append(g(w))
        weight_history.append(w)
        
    return weight_history, cost_history

def g(w: int) : return np.abs(w)

"""---

7) [10 points] Make a run of max_its=20 steps of gradient descent with initialization at the point $w^0 = 1.75$, and a fixed learning rate of $\alpha = 0.5$. Using the cost and weight history, plot the cost as a function of the weight for each step (cost on y-axis, weight on x-axis). Recall that the terms weight and parameter used interchangeably and both refer to w.
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

max_its = 20
weight_history, cost_history = gradient_descent(g, 0.5, max_its, 1.75)

plt.plot(weight_history, cost_history, color="green", label='cost vs. weight')

plt.title("Weight vs Cost")
plt.xlabel("Weight")
plt.ylabel("Cost")

plt.xticks(np.arange(-0.5, 2.5, 0.25))
plt.yticks(np.arange(-0.2, 2, 0.2))
plt.grid(linestyle="--")
plt.locator_params(nbins = max_its)
plt.legend(loc="lower right")
plt.locator_params(nbins = max_its)

plt.show()

"""---

8) [15 points] Make a run of max_its=20 steps of gradient descent with initialization at the point $w^0 = 1.75$, using the diminishing rule $\alpha = \frac{1}{k}$ (for this you have to modify the gradient_descent function slightly. Use the code template below. Using the cost and weight history, plot the cost as a function of the weight for each step (cost on y-axis, weight on x-axis)
"""

from autograd import grad
import autograd.numpy as np

def gradient_descent(g, alpha, max_its, w0):
    """
        Gradient Descent.\n
        `g`: cost function.
        `alpha`: learning rate. If "diminishing", use 1/step as alpha.
        `max_its`: number of iterations
        `w0`: initial weight 
        `returns`: lists of weights and corresponding costs
        costs after each iteration step.   
    """
    
    # 0. init
    gradient = grad(g)
    w = w0
    weight_history = [w]
    cost_history = [g(w)]
    
    # 1. iterate
    for step in range(max_its):

        # 2. step
        a = alpha if alpha != "diminishing" else 1 / (step + 1)
        w -= a * gradient(w)
        
        # 3. record
        cost_history.append(g(w))
        weight_history.append(w)
        
    return weight_history, cost_history


def g(w: int) : return np.abs(w)

# -----------------------------

max_its = 20
weight_history, cost_history = gradient_descent(g, "diminishing", max_its, 1.75)

plt.plot(weight_history, cost_history, color="green", label='cost vs. weight')

plt.title("Cost vs Weight")
plt.xlabel("Weight")
plt.ylabel("Cost")

plt.legend(loc="lower right")
plt.xticks(np.arange(0, 2.5, 0.25))
plt.yticks(np.arange(0, 2, 0.1))
plt.grid(linestyle="--")

plt.show()

"""---

9) [10 points]  Generate a single plot showing the cost as a function of step number for both runs (combine all  runs into a single plot). Which approach works better? Why?

- **Using $\alpha = 0.5$, the system converges to $g(w) \approx 0.25$.**
- **Using $\alpha = \frac{1}{k}$, the system appears to oscillate, implying that
it somehow overshoots the optimal value. However, it still attains better results than with $\alpha = 0.5$ &mdash; perhaps because the value of alpha is continually decreasing, the amount of overshoot grows smaller with more iteration, as do the oscillations. Repeated iteration (perhaps $2000$, or more) could achieve near-absolute convergence.**
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import autograd.numpy as np
# %matplotlib inline


max_steps = 20

steps = list(range(max_steps + 1))
_, cost_history1 = gradient_descent(g, 0.5, max_steps, 1.75)
_, cost_history2 = gradient_descent(g, "diminishing", max_steps, 1.75)

plt.plot(steps, cost_history1, color="green", label="alpha=0.5")
plt.plot(steps, cost_history2, color="red", label="alpha=diminishing")

plt.title("Cost vs Iteration")
plt.xlabel("Iteration")
plt.ylabel("Cost")

plt.legend(loc="upper right")
plt.xticks(steps)
plt.yticks(np.arange(0, 2, 0.1))
plt.grid(linestyle="--")

plt.legend()

plt.show()

"""We will now look at the oscilating behavior of gradient descent. 

Take the following cost function:
$g(w) = w_0^2 + w_1^2 + 2\sin(1.5 (w_0 + w_1)) +2$

Note that this cost function has two parameters.

---

10) [5 points] Make sure your gradient descent function from problem 6 can handle cost functions with more than one parameter. You may need to rewrite it if you were not careful. Use the code template below (if your function from problem 6 is good, you can just copy and paste it here)
"""

from autograd import grad, numpy as np
from math import sin

def gradient_descent(g, alpha, max_its, w0):
    """
        Gradient Descent.\n
        `g`: cost function.
        `alpha`: learning rate.
        `max_its`: number of iterations
        `w0`: initial weight 
        `returns`: lists of weights and corresponding costs
        costs after each iteration step.   
    """
    
    # 0. init
    gradient = grad(g)
    w = w0
    weight_history = [w]
    cost_history = [g(w)]
    
    # 2. iterate
    for _ in range(max_its):

        # 3. step
        w -= alpha * gradient(w)
        
        # 4. record
        cost_history.append(g(w))
        weight_history.append(w)
        
    return weight_history, cost_history

g = lambda w: w[0]**2 + w[1]**2 + 2*np.sin(1.5*(w[0] + w[1])) + 2

"""---

11) [10 points] Run the gradient_descent function with the cost function above three times with the following parameters. Generate a single plot showing the cost as a function of step number for all three runs (combine all three runs into a single plot). Use the code template below. Which alpha leads to an oscillating behavior? **$\alpha = 1$**

$w^0$ = [3.0,3.0]
max_its = 10

1. $\alpha = 0.01$ &mdash; Does not converge (over the number of iterations that we do) because the steps taken are too small.
2. $\alpha = 0.1$  &mdash; Converges after a few iterations.
3. $\alpha = 1$    &mdash; Displays oscillating behavior, possibly overshooting from one direction to the other because the higher value of $\alpha$ translates into bigger steps than optimal. 


"""

import autograd.numpy as np
import matplotlib.pyplot as plt
max_its = 10

steps = list(range(max_its + 1))
_, cost_history1 = gradient_descent(g, 0.01, max_its, np.array([3.0, 3.0]))
_, cost_history2 = gradient_descent(g, 0.1, max_its, np.array([3.0, 3.0]))
_, cost_history3 = gradient_descent(g, 1, max_its, np.array([3.0, 3.0]))

plt.plot(steps, cost_history1, color="green", label="alpha=0.01")
plt.plot(steps, cost_history2, color="blue", label="alpha=0.1")
plt.plot(steps, cost_history3, color="red", label="alpha=1")

plt.title("Performance with different learning rates")
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.legend(loc="upper right")
plt.yticks(range(0, 40, 5))
plt.xticks(steps)
plt.grid(linestyle="--")

plt.show()

"""---

12) [15 points] This problem is about learning to tune fixed step length for gradient descent. Here, you are given a cost function:
$g(w) = 2w_0^2 + w_1^2 +4w_2^2$ 

Assume your $w^0$= [5,5,5] and your max_iter = 100

Use your latest gradient descent function with a fixed learning rate. Play around with at least 5 different values of alpha (using your intuition). Generate a single plot of the cost as a function of the number of iterations. Which value of alpha seems to converge the fastest?

Not that your grade will not depend on how well you do, as long as you try at least 5 different values for alpha and plot them.

- **$\alpha = 0.1$ converges the fastest.**
- **$\alpha = 0.25$ starts converging as fast as $0.1$ but doesn't converge to the optimal value.**
- In general;
  - **$\alpha > 0.2$ causes the system to blow up and not converge.**
  - **$\alpha < 0.1$ causes slower convergence.**


"""

# Commented out IPython magic to ensure Python compatibility.
from autograd import grad 
import autograd.numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

# cost function
g = lambda x: 2*(x[0]**2) + x[1]**2 + 4*(x[2]**2)

max_its = 100
steps = list(range(max_its + 1))

weights, costs = gradient_descent(g, 0.001, max_its, np.array([5.0,5.0,5.0]))
plt.plot(steps, costs, color="purple", label="0.001")

weights, costs = gradient_descent(g, 0.01, max_its, np.array([5.0,5.0,5.0]))
plt.plot(costs, color="green", label="0.01")

weights, costs = gradient_descent(g, 0.05, max_its, np.array([5.0,5.0,5.0]))
plt.plot(steps, costs, color="crimson", label="0.05")

weights, costs = gradient_descent(g, 0.1, max_its, np.array([5.0,5.0,5.0]))
plt.plot(steps, costs, color="black", label="0.1")

weights, costs = gradient_descent(g, 0.25, max_its, np.array([5.0,5.0,5.0]))
plt.plot(steps, costs, color="blue", label="0.25")

plt.legend(loc="upper right")
plt.xticks(range(0, 110, 5))
plt.yticks(range(0, 190, 15))
plt.grid(linestyle="--")
plt.show()

"""---"""